{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOb6yGG4+w4OF6MQq1clXEs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"WUQaBQG8u1aI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666980805514,"user_tz":-120,"elapsed":21866,"user":{"displayName":"Artemis Capari","userId":"15724019396003198006"}},"outputId":"7f47e962-91b9-4dd7-d41c-f73e294ad03f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install sentence_transformers\n","!pip install rank_bm25\n","%cd /content/drive/MyDrive/IR2-TOMT-FINAL-VERSION"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aDCzqHKhvI0J","executionInfo":{"status":"ok","timestamp":1666980822105,"user_tz":-120,"elapsed":16602,"user":{"displayName":"Artemis Capari","userId":"15724019396003198006"}},"outputId":"00dcc333-055e-4751-a940-e0aaeae15442"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentence_transformers\n","  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n","\u001b[K     |████████████████████████████████| 85 kB 4.4 MB/s \n","\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n","  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n","\u001b[K     |████████████████████████████████| 5.3 MB 66.1 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.64.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.12.1+cu113)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.13.1+cu113)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.21.6)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.7.3)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.7)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 52.3 MB/s \n","\u001b[?25hCollecting huggingface-hub>=0.4.0\n","  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n","\u001b[K     |████████████████████████████████| 163 kB 66.9 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.13.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.8.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.1.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.23.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 38.9 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence_transformers) (3.9.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.2.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.0.4)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n","Building wheels for collected packages: sentence-transformers\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=368c60d27ea085a8fc3a2376f7086dcf14fd511fd53a8f7f6c4460736a87ed1c\n","  Stored in directory: /root/.cache/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\n","Successfully built sentence-transformers\n","Installing collected packages: tokenizers, huggingface-hub, transformers, sentencepiece, sentence-transformers\n","Successfully installed huggingface-hub-0.10.1 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.1 transformers-4.23.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting rank_bm25\n","  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rank_bm25) (1.21.6)\n","Installing collected packages: rank-bm25\n","Successfully installed rank-bm25-0.2.2\n","/content/drive/.shortcut-targets-by-id/1RjJy7kusAlC5nLryJ5ZJS_apRnHJhwwW/IR2-TOMT-FINAL-VERSION\n"]}]},{"cell_type":"code","source":["!python train.py --model_name sentence-transformers/all-MiniLM-L6-v2 --train_size 1000 --eval_size 1300 --test_size 1300 --train_eval_size 1000 --bm25_topk 100 --eval_batch_size 500 --evals_per_epoch 5 --loss_fn cos-sim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ACJ-flKuvVLd","executionInfo":{"status":"ok","timestamp":1666981197903,"user_tz":-120,"elapsed":375807,"user":{"displayName":"Artemis Capari","userId":"15724019396003198006"}},"outputId":"dd9719ee-d217-4e24-98b1-82741938a0a9"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Training setup:  {'model_name': 'sentence-transformers/all-MiniLM-L6-v2', 'loss_fn': 'cos-sim', 'epochs': 5, 'warmup_steps': 0, 'learn_rate': 2e-05, 'weight_decay': 0.01, 'batch_size': 10, 'eval_batch_size': 500, 'bm25_topk': 100, 'train_size': 1000, 'eval_size': 1300, 'test_size': 1300, 'train_eval_size': 1000, 'eval_steps': None, 'evals_per_epoch': 5, 'cross_encoder': None}\n","Downloading: 100% 1.18k/1.18k [00:00<00:00, 1.06MB/s]\n","Downloading: 100% 190/190 [00:00<00:00, 182kB/s]\n","Downloading: 100% 10.6k/10.6k [00:00<00:00, 7.45MB/s]\n","Downloading: 100% 612/612 [00:00<00:00, 533kB/s]\n","Downloading: 100% 116/116 [00:00<00:00, 99.4kB/s]\n","Downloading: 100% 39.3k/39.3k [00:00<00:00, 448kB/s]\n","Downloading: 100% 90.9M/90.9M [00:02<00:00, 44.9MB/s]\n","Downloading: 100% 53.0/53.0 [00:00<00:00, 46.9kB/s]\n","Downloading: 100% 112/112 [00:00<00:00, 106kB/s]\n","Downloading: 100% 466k/466k [00:00<00:00, 1.09MB/s]\n","Downloading: 100% 350/350 [00:00<00:00, 313kB/s]\n","Downloading: 100% 13.2k/13.2k [00:00<00:00, 9.54MB/s]\n","Downloading: 100% 232k/232k [00:00<00:00, 677kB/s]\n","Downloading: 100% 349/349 [00:00<00:00, 256kB/s]\n","STEPS PER EPOCH  404\n","EVALS STEPS  80\n","\n","Total training data:  4036 \n","Num train queries:  945 \n","Num eval queries:  1221 \n","Num test queries:  1236\n","CosineSimilarityLoss(\n","  (model): SentenceTransformer(\n","    (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n","    (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False})\n","    (2): Normalize()\n","  )\n","  (loss_fct): MSELoss()\n","  (cos_score_transformation): Identity()\n",")\n","Evaluating on training set..\n","Done, resuming training..\n","Evaluating on evaluation set..\n","New high score on eval set:  0.15597741305828094 . Saving best model..\n","Evaluating on evaluation set..\n","Evaluating on evaluation set..\n","Evaluating on evaluation set..\n","Evaluating on evaluation set..\n","Evaluating on training set..\n","Done, resuming training..\n","Evaluating on evaluation set..\n","Evaluating on evaluation set..\n","Traceback (most recent call last):\n","  File \"train.py\", line 136, in <module>\n","  File \"/content/drive/.shortcut-targets-by-id/1RjJy7kusAlC5nLryJ5ZJS_apRnHJhwwW/IR2-TOMT-FINAL-VERSION/sentence_transformers/SentenceTransformer.py\", line 734, in fit\n","    optimizer.step()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py\", line 65, in wrapper\n","    return wrapped(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\", line 113, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/optim/adamw.py\", line 175, in step\n","    capturable=group['capturable'])\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/optim/adamw.py\", line 231, in adamw\n","    capturable=capturable)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/optim/adamw.py\", line 266, in _single_tensor_adamw\n","    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n","KeyboardInterrupt\n"]}]},{"cell_type":"code","source":["!python train.py --model_name sentence-transformers/all-MiniLM-L6-v2 --train_size 1000 --eval_size 1300 --test_size 1300 --train_eval_size 1000 --bm25_topk 100 --eval_batch_size 500 --evals_per_epoch 5 --loss_fn multi-neg"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PKvcyqknvgxp","executionInfo":{"status":"ok","timestamp":1666981359358,"user_tz":-120,"elapsed":161465,"user":{"displayName":"Artemis Capari","userId":"15724019396003198006"}},"outputId":"a5cc16d4-d90e-4e2a-d0be-594bcee1bb45"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Training setup:  {'model_name': 'sentence-transformers/all-MiniLM-L6-v2', 'loss_fn': 'multi-neg', 'epochs': 5, 'warmup_steps': 0, 'learn_rate': 2e-05, 'weight_decay': 0.01, 'batch_size': 10, 'eval_batch_size': 500, 'bm25_topk': 100, 'train_size': 1000, 'eval_size': 1300, 'test_size': 1300, 'train_eval_size': 1000, 'eval_steps': None, 'evals_per_epoch': 5, 'cross_encoder': None}\n","test\n","STEPS PER EPOCH  403\n","EVALS STEPS  80\n","\n","Total training data:  4036 \n","Num train queries:  945 \n","Num eval queries:  1221 \n","Num test queries:  1236\n","MultipleNegativesRankingLoss(\n","  (model): SentenceTransformer(\n","    (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n","    (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False})\n","    (2): Normalize()\n","  )\n","  (cross_entropy_loss): CrossEntropyLoss()\n",")\n","Evaluating on training set..\n","Done, resuming training..\n","Evaluating on evaluation set..\n","New high score on eval set:  0.08075347542762756 . Saving best model..\n","Evaluating on evaluation set..\n","New high score on eval set:  0.12841607630252838 . Saving best model..\n","Evaluating on evaluation set..\n","New high score on eval set:  0.13485465943813324 . Saving best model..\n","Traceback (most recent call last):\n","  File \"train.py\", line 136, in <module>\n","    test_evaluator = test_evaluator\n","  File \"/content/drive/.shortcut-targets-by-id/1RjJy7kusAlC5nLryJ5ZJS_apRnHJhwwW/IR2-TOMT-FINAL-VERSION/sentence_transformers/SentenceTransformer.py\", line 708, in fit\n","    data = next(data_iterator)\n","  File \"/content/drive/.shortcut-targets-by-id/1RjJy7kusAlC5nLryJ5ZJS_apRnHJhwwW/IR2-TOMT-FINAL-VERSION/sentence_transformers/datasets/NoDuplicatesDataLoader.py\", line 41, in __iter__\n","    yield self.collate_fn(batch) if self.collate_fn is not None else batch\n","  File \"/content/drive/.shortcut-targets-by-id/1RjJy7kusAlC5nLryJ5ZJS_apRnHJhwwW/IR2-TOMT-FINAL-VERSION/sentence_transformers/SentenceTransformer.py\", line 553, in smart_batching_collate\n","    tokenized = self.tokenize(texts[idx])\n","  File \"/content/drive/.shortcut-targets-by-id/1RjJy7kusAlC5nLryJ5ZJS_apRnHJhwwW/IR2-TOMT-FINAL-VERSION/sentence_transformers/SentenceTransformer.py\", line 321, in tokenize\n","    return self._first_module().tokenize(texts)\n","  File \"/content/drive/.shortcut-targets-by-id/1RjJy7kusAlC5nLryJ5ZJS_apRnHJhwwW/IR2-TOMT-FINAL-VERSION/sentence_transformers/models/Transformer.py\", line 121, in tokenize\n","    output.update(self.tokenizer(*to_tokenize, padding=True, truncation='longest_first', return_tensors=\"pt\", max_length=self.max_seq_length))\n","  File \"/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\", line 2484, in __call__\n","    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\", line 2587, in _call_one\n","    **kwargs,\n","  File \"/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\", line 2778, in batch_encode_plus\n","    **kwargs,\n","  File \"/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_fast.py\", line 430, in _batch_encode_plus\n","    is_pretokenized=is_split_into_words,\n","KeyboardInterrupt\n"]}]},{"cell_type":"code","source":["!python train.py --model_name cross-encoder/ms-marco-TinyBERT-L-2-v2 --train_size 1000 --eval_size 10 --test_size 10 --train_eval_size 10 --bm25_topk 100 --eval_batch_size 500 --evals_per_epoch 5 --loss_fn multi-neg"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sQWvur98vmdl","executionInfo":{"status":"ok","timestamp":1666982015491,"user_tz":-120,"elapsed":182627,"user":{"displayName":"Artemis Capari","userId":"15724019396003198006"}},"outputId":"da4c71bf-ce17-49cf-efa1-f594192cd972"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Training setup:  {'model_name': 'cross-encoder/ms-marco-TinyBERT-L-2-v2', 'loss_fn': 'multi-neg', 'epochs': 5, 'warmup_steps': 0, 'learn_rate': 2e-05, 'weight_decay': 0.01, 'batch_size': 10, 'eval_batch_size': 500, 'bm25_topk': 100, 'train_size': 1000, 'eval_size': 10, 'test_size': 10, 'train_eval_size': 10, 'eval_steps': None, 'evals_per_epoch': 5, 'cross_encoder': None}\n","STEPS PER EPOCH  404\n","EVALS STEPS  80\n","\n","Total training data:  4036 \n","Num train queries:  9 \n","Num eval queries:  8 \n","Num test queries:  8\n","Making bm25 scores..\n","100% 9/9 [00:00<00:00, 10.37it/s]\n","Sorting scores and making the document set..\n","100% 9/9 [00:00<00:00, 235.24it/s]\n","bm25 docset size:  773\n","Making the indeces..\n","100% 9/9 [00:00<00:00, 131988.59it/s]\n","Making bm25 scores..\n","100% 8/8 [00:00<00:00, 10.04it/s]\n","Sorting scores and making the document set..\n","100% 8/8 [00:00<00:00, 233.17it/s]\n","bm25 docset size:  659\n","Making the indeces..\n","100% 8/8 [00:00<00:00, 136400.13it/s]\n","Making bm25 scores..\n","100% 8/8 [00:01<00:00,  6.95it/s]\n","Sorting scores and making the document set..\n","100% 8/8 [00:00<00:00, 239.56it/s]\n","bm25 docset size:  699\n","Making the indeces..\n","100% 8/8 [00:00<00:00, 134217.73it/s]\n","New best score:  0.1875 . Saving model..\n","Traceback (most recent call last):\n","  File \"train.py\", line 160, in <module>\n","    max_grad_norm=1.)\n","  File \"/content/drive/.shortcut-targets-by-id/1RjJy7kusAlC5nLryJ5ZJS_apRnHJhwwW/IR2-TOMT-FINAL-VERSION/sentence_transformers/cross_encoder/CrossEncoder.py\", line 236, in fit\n","    self._eval_during_training(evaluator, output_path, save_best_model, epoch, training_steps, callback)\n","  File \"/content/drive/.shortcut-targets-by-id/1RjJy7kusAlC5nLryJ5ZJS_apRnHJhwwW/IR2-TOMT-FINAL-VERSION/sentence_transformers/cross_encoder/CrossEncoder.py\", line 320, in _eval_during_training\n","    score = evaluator(self, output_path=output_path, epoch=epoch, steps=steps)\n","  File \"/content/drive/.shortcut-targets-by-id/1RjJy7kusAlC5nLryJ5ZJS_apRnHJhwwW/IR2-TOMT-FINAL-VERSION/sentence_transformers/evaluation/BM25IREvaluator.py\", line 102, in __call__\n","    metrics = self.compute_metrics(model)\n","  File \"/content/drive/.shortcut-targets-by-id/1RjJy7kusAlC5nLryJ5ZJS_apRnHJhwwW/IR2-TOMT-FINAL-VERSION/sentence_transformers/evaluation/BM25IREvaluator.py\", line 95, in compute_metrics\n","    sims = model.predict(combined, convert_to_tensor=True, batch_size=1000).reshape(self.num_queries, len(self.bm25_doc_texts)).detach()\n","  File \"/content/drive/.shortcut-targets-by-id/1RjJy7kusAlC5nLryJ5ZJS_apRnHJhwwW/IR2-TOMT-FINAL-VERSION/sentence_transformers/cross_encoder/CrossEncoder.py\", line 291, in predict\n","    for features in iterator:\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 681, in __next__\n","    data = self._next_data()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 721, in _next_data\n","    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n","    return self.collate_fn(data)\n","  File \"/content/drive/.shortcut-targets-by-id/1RjJy7kusAlC5nLryJ5ZJS_apRnHJhwwW/IR2-TOMT-FINAL-VERSION/sentence_transformers/cross_encoder/CrossEncoder.py\", line 99, in smart_batching_collate_text_only\n","    tokenized = self.tokenizer(*texts, padding=True, truncation='longest_first', return_tensors=\"pt\", max_length=self.max_length)\n","  File \"/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\", line 2484, in __call__\n","    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\", line 2587, in _call_one\n","    **kwargs,\n","  File \"/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\", line 2778, in batch_encode_plus\n","    **kwargs,\n","  File \"/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_fast.py\", line 430, in _batch_encode_plus\n","    is_pretokenized=is_split_into_words,\n","KeyboardInterrupt\n"]}]}]}